\documentclass{article}

\title{Artificial Intelligence assignment 2: Spam Filtering Using a Naive Bayes Text Classifier}
\author{Ilse Barf (S3500306), Galina Lesnic (S3367398), Nathan van Beelen (S3392961)}

\begin{document}

\maketitle

\section*{3.3 Example runs}
\textit{What happens if you train and test on the same data?}

The accuracy will be higher. This is due to the fact that it can calculate
correlations that are apparent in this particular set of the data but do not
define the difference between regular and spam mails. In other words, you can
safely overfit on the train data and get a high accuracy score. Additionally,
we used the vocabulary of the train data, which will be exactly the same in the
test if we use the same data for testing. This is not the case if we use a
different data set for testing.

\section*{4 Naive Bayes classification based on bigrams}

\textit{(a) How does the bigram classifier perform when compared to your original unigram classifier? Try to explain the difference in performance (if present).}

The bigram classifier performs better in all cases, as can be seen in the tables below.
\begin{table}[h]
    \centering
    \begin{tabular}{c|c c}
        Train/Test &Train&Test  \\
        \hline
        Train & 100\% & 91\% \\
        Test & 95\% & 100\%
    \end{tabular}
    \caption{Table of accuracies for unigram classifier.}
    \label{tab:my_label}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{c|c c}
        Train/Test &Train&Test  \\
        \hline
        Train& 100\%& 99\% \\
        Test & 100\% & 100\%
    \end{tabular}
    \caption{Table of accuracies for bigram classifier.}
    \label{tab:my_label}
\end{table}

This happens, because bigrams help us account more for the context in which the words occur.

\textit{(b) Try some different configurations of parameter settings ($\varepsilon$, frequency threshold, minimal word size). How do they influence the performance of the classifier? And what is the use of the frequency threshold in real-world applications?}

As $\varepsilon$ gets smaller, the system becomes more accurate, which makes sense, since we still want to consider those rarely occurring words very unlikely to be impactful. We decided to settle on the suggested by the assignment value of 1.

With frequency limit it's a bit trickier, since a value that is too small lets through bigrams that aren't too useful and if the frequency limit is too high it filters out too many bigrams, thus not leaving the flexibility for working with new messages. We decided to let through all the bigrams that occurred at least 6 times in total (chosen after trying out different parameter values).

Similar to the frequency limit, word size limit that is too small lets through words on the stop-list, but if the parameter is too high it blocks meaningful words from being considered. We decided on the limit size of 4, since it is fair to assume that majority of stop words (e.g. will, do, be, etc.) are of length 4 or less.

Frequency threshold is usually used in real-world applications to make the vocabulary sizes manageable, since n-gram vocabularies get exponentially bigger and it is fair to assume that the bigrams that are met very rarely are not too useful for categorizing information.

\section*{5 Final Questions}
\textit{The  data  used  in  this  assignment  contains  only  e-mails  in  the  English  language.
What happens if an e-mail in Dutch is given to your spam filter trained with English messages?
How  will  the  Dutch  message  be  classified?   Assume  that  there  are  no  common  words  in
English and Dutch.  Explain your answer.}
\begin{itemize}
    \item Nathan van Beelen: The messages will be classified according to the number of
          regular and spam messages used in the training data.
          This is because the classifier works by calculating the likelihood that a
          certain word is from a regular or spam mail. However, in Dutch it will not
          recognise any words. So this chance will be equal for both classifications.
          The only thing that differs is the chance that a mail is regular or spam based
          on the number of regular or spam mails. In the case this is also the same,
          the chances are equal. Due to the way we classify mails this means that
          mails will be classified as spam since the chance that it is regular needs
          to be bigger in order for it to be classified as regular.
    \item Ilse Barf: In this case, if there are an equal number of spam and regular messages
          (which makes the probability for encoutering one equal), the probability that the
          message is regular will be equal to the probability that the message is spam (it will be
          a probability very close to 0).
          Since the classifier will only classify an e-mail as regular if the probability that it
          is regular is strictly larger than the probability that it is spam, it will classify the
          e-mail as spam.
    \item Galina Lesnic: Assuming that there are no common words between English and Dutch, the spam filter will classify the e-mail as spam, since our classifier considers the e-mail regular only if it finds in the message more words associated with regular messages, than the ones associated with spam messages. Since there are no words associated with regular messages, 0 can’t be more than any non-negative number (here it will be also 0, so (0 $>$ 0) == False), the message will be classified as spam.
\end{itemize}

\textit{ The Naive Bayes assumption is that the attributes (or features) are independent.  Are the
words  in  a  message  really  independent?   And  what  can  you  say  about  the  independence
between and within bigrams?  Explain your answer in 250 words.}
\begin{itemize}
    \item Nathan van Beelen: No, the individual words are not independent. This is
                             because language has a certain structure. An adjective
                             needs a noun for example. Another example would be
                             semantics. The adjective 'new' would make sense in 
                             combination with the noun 'concept', but the adjective
                             'green' does not. This means that 'new' and 'concept'
                             have a semantic dependence, but 'green' and 'concept'
                             do not. Both in the case of between bigrams
                             and within bigrams there is a dependence. The dependence within
                             bigrams can be explained in a similar way as I did for the
                             individual words. Another example would be the determiner,
                             which will always be accompanied by either an adjective or
                             a noun on which it is dependent. In the case of the dependence
                             between bigrams it is mostly due to the dependence of words
                             to form a sentence (although the dependence can also be similar
                             to within bigrams if the words are split correctly).
                             To illustrate the dependence between bigrams, consider the
                             sentence 'Bob is playing soundly.' This will be split in:
                             'Bob is' and 'playing soundly.' The bigram 'Bob is' doesn't
                             make sense in and of itself. It needs a verb in order to make
                             sense. Therefore it is dependent on the bigram containg the
                             verb.
    \item Ilse Barf:         The words in a message are not likely to be independent. This
                             is caused firstly by the fact that a message typically has a
                             certain subject. Within the context of a subject certain words might
                             be either more or less common and either more or less likely to
                             appear together in the same message.

                             Another factor which causes dependece is the grammar of a language,
                             which forces words to appear together and sometimes even in a
                             specific order.

                             These arguments also hold for indepency between bigrams, since the
                             words that are next to each other in a message are not necessarily
                             dependent, so the dependency between bigrams is very to the dependency
                             between words.

                             Within bigrams, it is even more likely that the words are dependent,
                             since words like adjectives, adverbs, and prepositions that go with
                             certain verbs, are often found in close proximity of the word they go
                             with. However, this is not always the case, and since we do not
                             include words with less than four letters, the dependency will be
                             smaller than initially expected.
    \item Galina Lesnic: The words in the message are not actually independent, since there has to be some context. Different topics have different common words that are used, e.g. if you see word “kitchen”, you can guess that word “food” will also be in that text. Bigrams try to account for the context a bit, considering that phrases give more context, e.g. “save lives” is more likely to occur in regular e-mail, than “save money”. So the assumed (naive) independence between bigrams allows to decrease amount of calculations and dependence within them allows to be more accurate with context. 
\end{itemize}

\end{document}
